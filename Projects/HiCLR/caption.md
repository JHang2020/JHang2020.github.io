The overview architecture of the proposed **HiCLR**. There are *k* branches sharing the same query encoder weights corresponding to the hierarchical learning of different augmentations. We employ a growing augmentation strategy to generate multiple highly correlated positive pairs corresponding to different augmented strengths. The augmented view v<sub>i</sub> is fed into the query encoder f<sub>θ<sub>q</sub></sub> and the embedding projector h<sub>θ<sub>q</sub></sub> to obtain zi. Similarly, z′<sub>0</sub> is obtained by the key encoder f<sub>θ<sub>k</sub></sub> and the embedding projector h<sub>θ<sub>k</sub></sub>. Meanwhile, a hierarchical self-supervised loss is proposed to align the feature distributions of adjacent branches, which is optimized jointly with the InfoNCE loss.