<!DOCTYPE HTML>
<html class="no-js">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <title>PCM3 (ACM MM-23)</title>
    <meta name="description" content="Lithium Description" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <link href="css/plugins.css" media="screen" rel="stylesheet" type="text/css" />
    <link href="css/application.css" media="screen" rel="stylesheet" type="text/css" />
  </head>

<body>

    <!-- ABOUT -->

    <section id="page-about" class="section">
      <div align="center" style="padding-bottom: 100px;">
        <p class="copy-02">ACM MM 2023</p>
        <p class="heading h-01">Prompted Contrast with Masked Motion Modeling: <br>Towards Versatile 3D Action Representation Learning</p>

        <p class="copy-02">
            <a href="mailto:zjh2020@pku.edu.cn" class="links">Jiahang Zhang </a> &nbsp; &nbsp; 
            <a href="mailto:linlilang@pku.edu.cn" class="links">Lilang Lin </a> &nbsp; &nbsp; 
            <a href="mailto:liujiaying@pku.edu.cn" class="links">Jiaying Liu</a>
        </p>
      </div>

      <div class="site-inner">
        <h3 class="heading h-03">Abstract</h3>
            <p class="copy-02", style="text-align:justify; text-justify:inter-ideograph;">
            Self-supervised learning has proved effective for skeleton-based human action understanding, which is an important yet challenging topic. Previous works mainly rely on contrastive learning or masked motion modeling paradigm to model the skeleton relations. However, the sequence-level and joint-level representation learning cannot be effectively and simultaneously handled by these methods. As a result, the learned representations fail to generalize to different downstream tasks. Moreover, combining these two paradigms in a naive manner leaves the synergy between them untapped and can lead to interference in training. To address these problems, we propose <b>P</b>rompted <b>C</b>ontrast with <b>M</b>asked <b>M</b>otion <b>M</b>odeling, <b>PCM<sup>3</sup></b>, for versatile 3D action representation learning. Our method integrates the contrastive learning and masked prediction tasks in a mutually beneficial manner, which substantially boosts the generalization capacity for various downstream tasks. Specifically, masked prediction provides novel training views for contrastive learning, which in turn guides the masked prediction training with high-level semantic information. Moreover, we propose a dual-prompted multi-task pretraining strategy, which further improves model representations by reducing the interference caused by learning the two different pretext tasks. Extensive experiments on five downstream tasks under three large-scale datasets are conducted, demonstrating the superior generalization capacity of PCM<sup>3</sup> compared to the state-ofthe-art works.</p>
      </div>

      <br>

      <div class="site-inner">
        <h3 class="heading h-03">Key Idea</h3>
            <p class="copy-02", style="text-align:justify; text-justify:inter-ideograph;">(1) <i>Mutual Beneficial Design between the Contrastive Learning and Masked Prediction Tasks.</i> The novel views in masked prediction training are utilized  as the positive samples for contrastive learning. Meanwhile, contrastive learning provides semantic guidance for masked prediction in turn by propagating the gradients to the prediction decoder.
                <br>
            (2) <i> Dual-Prompted Pre-training Strategy.</i> Domain-specific
            prompts and task-specific prompts are introduced for the multi-task pretraining. These trainable prompts enable the model to achieve more discriminative representations for different skeletons.</p>
      </div>

      <br>

      <div class="site-inner" style="padding-top: 50px;">
        <h3 class="heading h-03">Framework</h3>
        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="img\arch.jpg" width=90%> <br>
        <p class='copy-02', style="text-align:justify; text-justify:inter-ideograph;">Figure 1. The overview of the proposed method. We integrate the masked skeleton prediction (blue part) and the contrastive learning (yellow-green part) paradigms in a mutually beneficial manner. For brevity, we represent intra- and inter- transformed views in a single branch in the diagram. The masked and the predicted views are utilized to expose more novel motion patterns for contrastive learning. Meanwhile, the gradients from contrastive learning (dotted arrows in figure) are propagated to the masked prediction branch to update the decoder. To further boost the representation learning from different views/tasks, we propose the dual-prompted multi-task pretraining strategy, where domain-specific and task-specific prompts are added in  input-wise and feature-wise form, serving as training guidance.
        </p>
        </div>
      </div>

      <div class="site-inner" style="padding-top: 50px;">
        <h3 class="heading h-03">Selected Experimental Results</h3>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
            <img src="img/linear.png" width="100%"> <br>
        <p class='copy-02'>Table 1. Linear evaluation results on NTU60 and NTU120 datasets.
        </div>
        <br>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
            <img src="img/retrieval.jpg" width="50%"> <br>
        <p class='copy-02'>Table 2. Action retrieval results on NTU60 and NTU120 datasets.
        </p>
        </div>
        <br>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
            <img src="img/occ.jpg" width="50%"> <br>
            <p class='copy-02'>Table 3. Action recognition results with occlusion on NTU60 dataset.
            </p>
        </div>

        <br>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
            <img src="img/detection.jpg" width="50%"> <br>
            <p class='copy-02'>Table 4. Action detection results on PKUMMD datasets.
            </p>
        </div>
        



        



      </div>

      <div class="site-inner" style="padding-top:50px;">
        <p class="heading h-03"> Resources </p> 
          <ul style="line-height:1.5; padding-left: 50px; padding-right: 50px">
             <li class="copy-02"> Paper: <a href="">arXiv</a> </li>
             <li class="copy-02"> Code: Coming Soon </li>
          </ul>
      </div>
      

      <div class="site-inner" style="padding-top:50px;">
        <p class='heading h-03'> Citation</p>
        <p class="copy-02"> @InProceedings{PCM_zhang2023, <br>
        &nbsp; &nbsp; author = {Zhang, Jiahang and Lin, Lilang and Liu, Jiaying}, <br>
        &nbsp; &nbsp; title = {Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning}, <br>
        &nbsp; &nbsp; booktitle = {Proceedings of the ACM International Conference on Multimedia}, <br>
        &nbsp; &nbsp; year = {2023} <br>
        } <br> 
        </p>
      </div>

      <div class="site-inner" style="padding-top:50px;">
        <p class="heading h-03"> Reference </p> 
	<p class="copy-02"> 
		[1] Liu, J.; Song, S.; Liu, C.; Li, Y.; and Hu, Y. A benchmark dataset and comparison study for multi-modal human action analytics. <i>TOMM</i> 2020. <br>
		[2] Shahroudy, A.; Liu, J.; Ng, T.-T.; and Wang, G. NTU RGB + D: A large scale dataset for 3d human activity analysis. <i>CVPR</i> 2016. <br> 
		[3] Liu, J.; Shahroudy, A.; Perez, M.; Wang, G.; Duan, L.-Y.; and Kot, A. C. NTU RGB + D 120: A large-scale benchmark for 3D human activity understanding. <i>TPAMI</i> 2019. <br>
		[4] Guo, T.; Liu, H.; Chen, Z.; Liu, M.; Wang, T.; and Ding, R. Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition. <i>AAAI</i> 2022. <br>
		[5] Lin, L.; Song, S.; Yang, W.; and Liu, J. MS2L: Multi-task self-supervised learning for skeleton based action recognition. <i>ACM MM</i> 2020.
	</p>
    </div>

	
    <div class="site-inner" style="padding-top:20px;">
    <p class="copy-02">
        <hr />
		<ul>
			Return to the <a href="http://39.96.165.147/Projects.html" class="links">STRUCT Project</a>
		</ul>
	</p>
    </div>

    <section id="page-about" class="section">

</body>
</html>