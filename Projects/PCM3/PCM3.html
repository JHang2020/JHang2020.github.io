<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>HiCLR-AAAI2023</title>
    <style type="text/css">
        body{
        	background-color: white;
        }
        .links{
        	text-decoration: none;
        	color: #0066CC;
        }
        .p2{
        	padding-top: 20px;
        	font-size: 25px;
        }
        .p1{
        	text-align:justify;
        	text-justify:inter-ideograph;
        }
		
		.left {
			text-align: left;
			border: 1px dotted black;
			width: 50%;
		}
        a{
        	font-family: Sans-serif;
        }
        p{
        	font-family: Sans-serif;
        }
        ul{
        	font-family: Sans-serif;
        }
    </style>
</head>
<body>
	<div align="center" style="padding-top: 30px;">
	<p style="font-size:35px;">Prompted Contrast with Masked Motion Modeling: Towards <br>Versatile 3D Action Representation Learning</p>

	<a href="mailto:zjh2020@pku.edu.cn" class="links">Jiahang Zhang </a> &nbsp; &nbsp; 
	<a href="mailto:linlilang@pku.edu.cn" class="links">Lilang Lin </a> &nbsp; &nbsp; 
	<a href="mailto:liujiaying@pku.edu.cn" class="links">Jiaying Liu</a>

	<br>
	<p class="para-3"><span class="p1"> Wangxuan Institute of Computer Technology, Peking University</span></p>
	<p class="para-3"><span class="p1"> Accepted by <i>ACM MM 2023.</i></span></p>
	

	</div>

        <div align="left" style="padding-left: 15%; padding-right: 15%; padding-bottom: 30px;">
		<p class='p2'> Abstract </p> 
		<p class='p1', style="line-height:140%">Self-supervised learning has proved effective for skeleton-based human action understanding, which is an important yet challenging topic. Previous works mainly rely on contrastive learning or masked motion modeling paradigm to model the skeleton relations. However, the sequence-level and joint-level representation learning cannot be effectively and simultaneously handled by these methods. As a result, the learned representations fail to generalize to different downstream tasks. Moreover, combining these two paradigms in a naive manner leaves the synergy between them untapped and can lead to interference in training. To address these problems, we propose <b>P</b>rompted <b>C</b>ontrast with <b>M</b>asked <b>M</b>otion <b>M</b>odeling, <b>PCM<sup>3</sup></b>, for versatile 3D action representation learning. Our method integrates the contrastive learning and masked prediction tasks in a mutually beneficial manner, which substantially boosts the generalization capacity for various downstream tasks. Specifically, masked prediction provides novel training views for contrastive learning, which in turn guides the masked prediction training with high-level semantic information. Moreover, we propose a dual-prompted multi-task pretraining strategy, which further improves model representations by reducing the interference caused by learning the two different pretext tasks. Extensive experiments on five downstream tasks under three large-scale datasets are conducted, demonstrating the superior generalization capacity of PCM<sup>3</sup> compared to the state-ofthe-art works.</p>
            </div>
			
        <div align="left" style="padding-left: 15%; padding-right: 15%; padding-bottom: 30px;">
            <p class='p2'> Method </p> 
			<p class='p1', style="line-height:140%">
			<strong>Key Idea</strong>: 
			<br>
			(1) <i>Mutual Beneficial Design between the Contrastive Learning and Masked Prediction Tasks.</i> The novel views in masked prediction training are utilized  as the positive samples for contrastive learning. Meanwhile, contrastive learning provides semantic guidance for masked prediction in turn by propagating the gradients to the prediction decoder.
			<br>
			(2) <i> Dual-Prompted Pre-training Strategy.</i> Domain-specific
			prompts and task-specific prompts are introduced for the multi-task pretraining. These trainable prompts enable the model to achieve more discriminative representations for different skeletons.
			</p>
			
			<br>
            <div style="padding-left: 5%; padding-right: 5%;">
                <div align="center">
                    <img src="img/arch.jpg" width="100%"> <br>
                </div>
            <p style="line-height:180%">Figure 1. The overview of the proposed method. We integrate the masked skeleton prediction (blue part) and the contrastive learning (yellow-green part) paradigms in a mutually beneficial manner. For brevity, we represent intra- and inter- transformed views in a single branch in the diagram. The masked and the predicted views are utilized to expose more novel motion patterns for contrastive learning. Meanwhile, the gradients from contrastive learning (dotted arrows in figure) are propagated to the masked prediction branch to update the decoder. To further boost the representation learning from different views/tasks, we propose the dual-prompted multi-task pretraining strategy, where domain-specific and task-specific prompts are added in  input-wise and feature-wise form, serving as training guidance.
            
			</div>
        
            <p class='p2'> Results </p> 
            <div style="padding-left: 5%; padding-right: 5%;">
                <div align="center">
					<p style="line-height:150%">
					Table 1. Linear evaluation results on NTU60 and NTU120 datasets.
					</p>
                    <img src="img/linear.png" width="100%"> <br>
                </div>
			<br>

                <div align="center">
					<p style="line-height:150%">
					Table 2. Action retrieval results on NTU60 and NTU120 datasets.
					</p>
                    <img src="img/retrieval.jpg" width="50%"> <br>
                </div>

				<div align="center">
					<p style="line-height:150%">
					Table 3. Action recognition results with occlusion on NTU60 dataset.
					</p>
                    <img src="img/occ.jpg" width="50%"> <br>
                </div>

				<div align="center">
					<p style="line-height:150%">
					Table 3. Action detection results on PKUMMD datasets.
					</p>
                    <img src="img/detection.jpg" width="50%"> <br>
                </div>
			</div>
		

	<p class='p2'> Resources </p> 
	<p class='p1'>
		<ul style="line-height:15px">
		　　<li> Paper: <a href="" class="links">arXiv</a> </li>
		　　<!--<li> Supplementary: <a href="https://drive.google.com/open?id=1QDMWbhw2jgutDsLaS00R-P6cxZ_OaZis" class="links">Google Drive</a>, <a href="https://pan.baidu.com/s/1ke9hqo62pVhl6_6YqAA4cQ" class="links">Baidu Pan</a> (Code: wvr6) </li>-->

		　　<li> <a href="" class="links">Code to be released</a> </li>
		</ul>
	</p>

	<p class='p2'> Citation</p>
	<p> 
		@article{PCM_zhang2023, <br>
			&nbsp; &nbsp; title={Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning},<br>
			&nbsp; &nbsp; author={Zhang, Jiahang and Lin, Lilang and Liu, Jiaying},<br>
			&nbsp; &nbsp; booktitle ={Proceedings of the ACM International Conference on Multimedia},<br>
			&nbsp; &nbsp; year={2023}, <br>
			}
	</p>

	<p class='p2'> Reference</p>
	<p> 
		[1] Liu, J.; Song, S.; Liu, C.; Li, Y.; and Hu, Y. A benchmark dataset and comparison study for multi-modal human action analytics. <i>TOMM</i> 2020. <br> <br>
		[2] Shahroudy, A.; Liu, J.; Ng, T.-T.; and Wang, G. Ntu rgb+ d: A large scale dataset for 3d human activity analysis. <i>CVPR</i> 2016. <br> <br>
		[3] Liu, J.; Shahroudy, A.; Perez, M.; Wang, G.; Duan, L.-Y.; and Kot, A. C. NTU RGB + D 120: A large-scale benchmark for 3D human activity understanding. <i>TPAMI</i> 2019. <br> <br>
		[4] Guo, T.; Liu, H.; Chen, Z.; Liu, M.; Wang, T.; and Ding, R. Contrastive Learning from Extremely Augmented Skeleton Sequences for Self-supervised Action Recognition. <i>AAAI</i> 2022. <br> <br>
		[5] Lin, L.; Song, S.; Yang, W.; and Liu, J. MS2L: Multi-task self-supervised learning for skeleton based action recognition. <i>ACM MM</i> 2020.
	</p>
	
	<p class='left'>
		<ul style="line-height:15px">
			Return to the <a href="http://39.96.165.147/Projects.html" class="links">STRUCT Project</a>
		</ul>
	</p>
		

</html>
